[{"categories":null,"contents":"Gaussian Process, or GP for short, is an underappreciated yet powerful algorithm for machine learning tasks. It is a non-parametric, Bayesian approach to machine learning that can be applied to supervised learning problems like regression and classification. Compared to other supervised learning algorithms, GP has several practical advantages: it can work well on small datasets and has the ability to provide uncertainty measurements on the predictions.\nIn this tutorial, I am going to demonstrate how to perform GP regression using GPyTorch. GPyTorch is a Gaussian process library implemented using PyTorch that is designed for creating scalable and flexible GP models. You can learn more about GPyTorch on their official website.\n Note: This tutorial is not necessarily intended to teach the mathematical background of GP, but rather how to build one using GPyTorch. I highly recommend reading the Chapter 2 of Gaussian Processes for Machine Learning for a very thorough introduction to GP regression.\n \r\r\rSetup\r\rBefore we start, we first need to install the gpytorch library. You can do this either by pip or conda using the following command:\n1 2 3 4  # Install using pip pip install gpytorch # Install using conda conda install gpytorch -c gpytorch   You can also check the requirements and installation instructions on their website here.\n Note: If you want to follow along with this tutorial, you can find the notebook of this tutorial here.\n Generating the data Next, we need to generate a training data for our model. We will be modeling the following function:\n$$y = \\sin{(2 \\pi x)} + \\epsilon, \\enspace \\epsilon \\sim \\mathcal{N}(0, 0.04)$$\nThe above function is the true function for our GP model, which is a sine function with Gaussian noise. We will evaluate this function on 15 equally-spaced points from [0,1]. The generated training data is depicted in the following plot:\n\r\r\rBuilding the model\r\rNow that we have generated our training data, we can start building our GP model. GPyTorch offers a flexible way for us to build GP models, by constructing the components of the model by ourselves. This is analogous to building neural networks in the standard PyTorch library. For most GP regression models, you will need to construct the following components:\n A GP Model: For exact (i.e. non-variational) GP models we will use gpytorch.models.ExactGP. A likelihood function: The likelihood function for GP regression, we commonly use gpytorch.likelihoods.GaussianLikelihood. A mean function: The prior mean of the GP. If you don’t know which mean function to use, gpytorch.means.ConstantMean() is usually a good place to start. A kernel function: The prior covariance of the GP. We’ll use the Spectral Mixture (SM) kernel (gpytorch.kernels.SpectralMixtureKernels()) for this tutorial. A multivariate normal distribution: The multivariate normal distribution in GP (gpytorch.distributions.MultivariateNormal)  We can build our GP model by constructing the above components as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class SpectralMixtureGP(gpytorch.models.ExactGP): def __init__(self, x_train, y_train, likelihood): super(SpectralMixtureGP, self).__init__(x_train, y_train, likelihood) self.mean = gpytorch.means.ConstantMean() # Construct the mean function self.cov = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4) # Construct the kernel function self.cov.initialize_from_data(x_train, y_train) # Initialize the hyperparameters from data def forward(self, x): # Evaluate the mean and kernel function at x mean_x = self.mean(x) cov_x = self.cov(x) # Return the multivariate normal distribution using the evaluated mean and kernel function return gpytorch.distributions.MultivariateNormal(mean_x, cov_x) # Initialize the likelihood and model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = SpectralMixtureGP(x_train, y_train, likelihood)   Let me breakdown the above code line-by-line:\n The above GP model has two main components: the __init__ and forward method. The __init__ method takes the training data and a likelihood as the inputs and constructs whatever objects are necessary for the model’s forward method. This will most commonly include objects like a mean function and a kernel function. The forward method takes in the data x and returns a multivariate normal distribution with the prior mean and covariance evaluated at x. Finally, we initialize the likelihood function for the GP model. Here, we use the Gaussian likelihood, which is the simplest likelihood function that assumes a homoskedastic noise model (i.e. all inputs have the same noise).  \r\r\rTraining the model\r\rNow that we have built the model, we can train the model to find the optimal hyperparameters. Training a GP model in GPyTorch is also analogous to training a neural network in the standard PyTorch library. The training loop mainly consists of the following steps:\n Setting all the parameter gradients to zero Calling the model and computing the loss Calling backward on the loss to fill in gradients Taking a step on the optimizer   Note: By defining our custom training loop, we can have greater flexibility in training our model. For example, it is easy to save the parameters at each step of training or use different learning rates for different parameters.\n The code for the training loop is given below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # Put the model into training mode model.train() likelihood.train() # Use the Adam optimizer, with learning rate set to 0.1 optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # Use the negative marginal log-likelihood as the loss function mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) # Set the number of training iterations n_iter = 50 for i in range(n_iter): # Set the gradients from previous iteration to zero optimizer.zero_grad() # Output from model output = model(x_train) # Compute loss and backprop gradients loss = -mll(output, y_train) loss.backward() print(\u0026#39;Iter %d/%d- Loss: %.3f\u0026#39; % (i + 1, n_iter, loss.item())) optimizer.step()   In the above code, we first put our model into training mode by calling model.train() and likelihood.train(). Then, we define the loss function and optimizer that we want to use in the training process. Here, we use the negative marginal log-likelihood as the loss function and Adam as the optimizer. We also need to set the number of iterations for the training loop, say 50 iterations.\n\r\r\rMaking predictions with the model\r\rFinally, we can make predictions using the trained model. The basic routine of evaluating the model and making predictions is given in the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # The test data is 50 equally-spaced points from [0,5] x_test = torch.linspace(0, 5, 50) # Put the model into evaluation mode model.eval() likelihood.eval() # The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances) # See https://arxiv.org/abs/1803.06058 with torch.no_grad(), gpytorch.settings.fast_pred_var(): # Obtain the predictive mean and covariance matrix f_preds = model(x_test) f_mean = f_preds.mean f_cov = f_preds.covariance_matrix # Make predictions by feeding model through likelihood observed_pred = likelihood(model(x_test)) # Initialize plot f, ax = plt.subplots(1, 1, figsize=(8, 6)) # Get upper and lower confidence bounds lower, upper = observed_pred.confidence_region() # Plot training data as black stars ax.plot(x_train.numpy(), y_train.numpy(), \u0026#39;k*\u0026#39;) # Plot predictive means as blue line ax.plot(x_test.numpy(), observed_pred.mean.numpy(), \u0026#39;b\u0026#39;) # Shade between the lower and upper confidence bounds ax.fill_between(x_test.numpy(), lower.numpy(), upper.numpy(), alpha=0.5) ax.set_ylim([-3, 3]) ax.legend([\u0026#39;Observed Data\u0026#39;, \u0026#39;Mean\u0026#39;, \u0026#39;Confidence\u0026#39;])   There are a few things going on in the above code:\n We first generate the test data using 50 equally-spaced points from [0, 5]. We put the model into evaluation mode by calling model.eval() and likelihood.eval(). We use gpytorch.settings.fast_pred_var() to get faster predictive distributions using LOVE. When put into the eval mode, the trained GP model returns a MultivariateNormal containing the posterior mean and covariance. Thus, we can obtain the predictive mean and covariance matrix from the multivariate normal distribution. Finally, we plot the mean and confidence region of the fitted GP model. The confidence_region() method is a helper method that returns 2 standard deviations above and below the mean.  The resulting plot is depicted below:\nThe black stars in the above plot represent the training (observed) data, while the blue line and the shaded area represent the mean and the confidence bounds respectively. Notice how the uncertainty is reduced close to the observed points. If more data points were added, we would see the mean function adjust itself to pass through these points and the uncertainty would reduce close to the observations.\n\r\r\rTakeaways\r\rIn this tutorial, we have learned how to build a scalable and flexible GP model using GPyTorch. Like any other libraries, there are still a lot of cool things that you can do with GPyTorch which I didn’t cover in this tutorial. For example, you can utilize a GPU to accelerate your model training when implementing state-of-the-art algorithms like Deep GP or stochastic variational deep kernel learning.\n","date":"2021-09-21","permalink":"https://richardcsuwandi.github.io/blog/posts/gp-regression/","tags":["gaussian-process","machine-learning","data-science"],"title":"Gaussian Process Regression using GPyTorch"},{"categories":null,"contents":"As technology advances, AI is becoming more and more integrated into our lives. Numerous industries and companies are using AI to improve their products and services. For instance, Google uses machine learning to build personalized services and maximize user experiences. Unfortunately, it also comes with a challenge as such models need to be trained on tons of personal data in order to perform well. This certainly becomes an issue as we are increasingly concerned about privacy. Here\u0026rsquo;s where federated learning comes to play.\nIn this blog post, I will introduce the concept of federated learning, why it matters, and how it could potentially shape the future of AI.\n\r\r\rWhat is federated learning?\r\rFederated learning was first introduced by Google in 2017, in a blog post titled \u0026ldquo;Federated Learning: Collaborative Machine Learning without Centralized Training Data\u0026rdquo;. As they put it,\n \u0026ldquo;Federated learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud.\u0026rdquo;\n Thus, federated learning can be thought of as a new approach to machine learning that allows you to train models across devices without pooling the data. Instead of bringing users' data to the server, federated learning brings the machine learning model to the users' devices. With the above idea in mind, the way federated learning works is actually quite intuitive. A general federated learning framework usually involves the following steps:\n Your device downloads the current model from the server and improves it by training from data on your device Your device uploads the training results in the form of a small focused update to the server using encrypted communication The server averages the updates across all users to improve the shared model  Here, it is worth noting that federated learning is an iterative process; which means that the above steps will be repeated until a pre-defined termination criterion is met (e.g., a maximum number of iterations is reached or the model accuracy is greater than a threshold). Note: If you\u0026rsquo;re interested to learn more about how federated learning works on a technical level, you can check out this 2016 paper published by Google AI researchers. If you prefer a less technical overview, you should check out this online comic instead.\n\r\r\rWhy does federated learning matter?\r\rTraditional machine learning adopts a centralized approach which requires all the data to be brought together to a server, where the models are trained. This centralized training approach, however, is privacy-intrusive as users have to trade their privacy by sending their personal data to the server owned by the AI companies. In light of this, federated learning is basically the decentralized form of machine learning. Unlike the traditional machine learning approaches, federated learning does not require the data to be stored in the server. Instead, the data stays on each users' device and thus privacy is preserved. In such a case, users can benefit from obtaining a well-trained machine learning model without sending their sensitive personal data to the server.\n\r\r\rHow could federated learning shape the future of AI?\r\rIn recent years, data privacy has become the major obstacle for the development of AI, especially with the establishment of data protection and privacy laws like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). Thanks to federated learning, these barriers have been broken and we now have a secure way of training machine learning models within users' devices.\nFederated learning allows us to build better machine learning models that can provide personalized services and maximize user experiences. A huge amount of data are produced from user\u0026rsquo;s devices on a daily basis. These data are valuable as it contains personal information about the users and their personal interests. With federated learning, it is now possible to build such personalized models while still preserving users' privacy.\n\r\r\rWhat\u0026#39;s Next?\r\rIn this blog post, we have explored a new decentralized approach to machine learning called federated learning. I personally believe that federated learning will play an important part in shaping the future of AI. In the near future, we will see a plethora of new applications taking advantage of federated learning, enhancing user experience in a way that was not possible before. Of course, this breakthrough also comes with a unique set of challenges that AI researchers need to tackle to bring this field forward. Nevertheless, the future looks exciting if AI and privacy could go hand in hand.\n","date":"2021-08-25","permalink":"https://richardcsuwandi.github.io/blog/posts/federated-learning/","tags":["federated-learning","machine-learning","artificial-intelligence"],"title":"Why the Future of AI is Federated"},{"categories":null,"contents":"\r\r\rMotivation\r\rAs an active user on LinkedIn with more than 1000 connections, I was curious about the statistics of my network. In particular, I was wondering about these questions:\n What are the segments of people in my network?\n  Do most of the people in my network work in data science related field?\n In this blog post, I will go through how I utilized exploratory analysis and data visualizations to answer these questions and gain insights from my own LinkedIn data. The notebook of this project can be found here\n\r\r\rData\r\rTo analyze and visualize my network, the first step that I need to do is to get my LinkedIn data. All I need to do is just select which data I want to get and it will be downloaded as a CSV file. Here are the first few rows from the Connections.csv file that I\u0026rsquo;ve downloaded: The DataFrame above contains data about the current positions and companies of people in my network as well as when the date I connect to that person.\n\r\r\rKey Findings\r\r\r\r\rNumber of Connections on a Given Date\r\r How many connections did I make in a given date?\n From the line plot above, we can see that there is a peak in the number of connections per day on 26 August 2020. It also seems that August 2020 is the period when I was the most active on LinkedIn.\n\r\r\rTop Companies/Organizations in my Network\r\r Which companies/organizations do the people in my network mainly come from?\n Using the treemap above, it is easier to compare the proportion of one company/organization to the others. It looks like the largest proportion of my network are from my university.\n\r\r\rTop Positions in my Network\r\r What are the top common positions of people in my network?\n The top position in my network is data scientists, followed by machine learning engineers and data analysts. It is great to know that the top common positions in my network are my target group for networking.\n","date":"2021-02-24","permalink":"https://richardcsuwandi.github.io/blog/posts/linkedin-analysis/","tags":["data-analysis","data-science","data-visualizations"],"title":"Analyzing My Own LinkedIn Data"},{"categories":null,"contents":"Have you ever wondered why AI has such an easy time doing stuff that we find very hard? But having a hard time doing stuff that we find very easy?\n\r\r\rMoravec\u0026#39;s Paradox\r\rIn the 1980s, an AI researcher named Hans Moravec wondered the exact same thing. As Moravec put it:\n \u0026ldquo;It is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.\u0026rdquo;\n This curious observation was later known as Moravec\u0026rsquo;s Paradox. And even though it was formulated more than 30 years ago, it is still relevant today.\nDon't get me wrong, AI certainly has come a long way. We've seen AI beating the world champion in the board game Go, the quiz game Jeopardy, the card game Poker, and the video game Dota 2. But on the flip side, AI is still having a hard time understanding a joke or interpreting people\u0026rsquo;s emotions. Therefore, the big question is:\n \u0026ldquo;Why does AI struggle with the simple?\u0026rdquo;\n \r\r\rThe reason behind Moravec\u0026#39;s Paradox\r\rAt the most basic level, the reason for Moravec's Paradox is simple: We don't know how to program general intelligence (yet). We're already good at getting AI to do specific things, but most toddler level skills require learning new things and transferring them into different contexts. And to get machines to do this is actually one of the goals of Artificial General Intelligence (AGI).\n \u0026ldquo;Today's AI is brilliant at very narrow competencies, whereas humans are good at pretty much everything.\u0026rdquo; — Dr. Sean Holden, Cambridge University\n Moravec also pointed out that the simple reason behind this is evolution. Things that seem easy to us are actually the product of thousands of years of evolution. In other words, they only seem easy to us because our species have spent thousands of years refining them.\n \u0026ldquo;The oldest human skills are largely unconscious and they appear to us to be effortless. Consequently, it should not be a surprise that the skills that appear effortless, to be computation-heavy and difficult to be reverse-engineered by a man-made AI system.\u0026rdquo;\n Moreover, the only way that we can teach an AI is by giving it a set of instructions to do a certain task. And since we've learned consciously how to do maths and win games, we know the exact steps needed to complete these tasks. Thus, we are able to teach these to an AI.\nBut how can you teach an AI to actually see, hear, or smell something? We don't know all the steps required to do these tasks consciously. In fact, we need to break these tasks into logical steps to feed into an AI. Hence, it is incredibly difficult to teach these to an AI.\n\r\r\rWhat Moravec\u0026#39;s Paradox actually taught us\r\rMoravec's Paradox has surely proven to us one thing— the fact that we've developed an AI that has beaten humans in Go or Chess doesn't mean that AGI is just around the corner. But yes, we are one step closer. It also shows why adult-level reasoning capable AI is an old hat, but AI with vision, listening, and learning capabilities are new and exciting. Of course, things are shifting, as AI is overcoming the Moravec's Paradox. More advanced AI is starting to mimic our evolutionary abilities. For instance, we've seen advancements in the Computer Vision field such as object detection and facial recognition, which could be thought of as the equivalent of sight for a computer. Also, thanks to Natural Language Processing (NLP), we now have personal assistants like Alexa that are capable of ‘hearing' and understanding us. Likewise, AI is becoming capable of speech, like we've seen in these assistants or developments like Google Duplex.\n\r\r\rThe impact of Moravec\u0026#39;s Paradox and the future of AI\r\rWhile the ultimate goal of achieving AGI remains elusive, Moravec's Paradox has brought a significant impact on our present world. Contrary to the traditional assumptions, it suggests that reasoning which is high-level in humans requires very little computational power. On the other hand, sensorimotor skills which are comparatively low-level in humans require enormous computational power. With this in mind, as computational power increases, machines could eventually match and exceed human capability.\nUltimately, AI has seen highs and lows. It's a field that is constantly saturated with ethical questions and scientific challenges. And although research on multitasking machines and AI with transferable skills is heating up, the debate remains on whether true human-level intelligence in machines is feasible (or desirable).\n \u0026ldquo;No computer has ever been designed that is ever aware of what it's doing; but most of the time, we aren't either.\u0026rdquo; — Marvin Minsky\n ","date":"2020-08-30","permalink":"https://richardcsuwandi.github.io/blog/posts/moravec/","tags":["artificial-intelligence","machine-learning","technology"],"title":"Why Is AI So Smart and Yet So Dumb?"},{"categories":null,"contents":"When I was a kid, I used to love playing with Lego. My brother and I built almost all kinds of stuff with Lego — animals, cars, houses, and even spaceships. As time went on, our creations became more ambitious and realistic. There were also times when we could each have insisted that our Lego was our own, till we realized that pooling resources would eventually help us went further. We were growing up too, and as our playing became more sophisticated, we learned how to make better models.\nAs an aspiring data scientist, I realized that working with data is surprisingly a lot like my childhood Lego memories. In this blog post, I want to share some of the memories I've had that show how playing with Lego and working with data are closer than you think.\n\r\r\rExploration is the most fun part of the process\r\rWhen I was a kid, I liked to put all my Lego bricks together in a giant tub because a lot of fun in building something was searching through a sea of bricks and trying out new patterns that I didn't think about before. Anyone who deals with data knows that as much as 80% of the process is cleaning up the data and doing exploratory analysis. Personally, that's what I love about working with data — that's where I let my creativity and imagination run wild. Jumping straight into the dataset and exploring various visualizations and correlations, in search of patterns, brings me back to a childhood spent digging through a pile of Lego.\n\r\r\rTo build something useful you need lots of resources\r\rIf you don't have enough Lego bricks, chances are the things you're building aren't realistic. The model is crude, the colors don't match, and there are gaps. The same goes for machine learning models. If you don't have enough data, your models are poor, and you will encounter lots of errors.\nHowever, sometimes, I might not have the right pieces to build a model exactly the way I wanted it, so I had to search for alternatives or reconsider how to build my Lego model. Hence, I learned a new way of using what I had. Similarly, as long as you are creative about where you look, there are always insights to be gained from even the most limited data.\n\r\r\rA good quality model needs a diversity of resources\r\rTo build a good quality Lego model, you also need a diversity of bricks. Models built with only the basic 2x4 bricks are rough and inaccurate. This is where it was so useful to get Lego from friends and family. As our family and friends gave us more Lego bricks, we got more diverse bricks that helped us create more accurate models.\nThis may also be a harsh childhood truth, that the children with the most Lego, the best pieces, and the most time to play create the best models. The same harsh truth applies to any machine learning projects. Projects with the biggest data volumes, the most diverse data, and the best teams to use the data would create the most accurate models.\n\r\r\rBoth require iterative thinking\r\rThe beauty of Lego is that you're not limited to what's on the box. Rebuilding something and refining it each time requires iterative thinking. When it comes to working with data, there are also plenty of opportunities to iterate.\nWhen I get a \u0026ldquo;decent enough\u0026rdquo; solution, whether it's a dashboard or a Python script, I still find time to break it, repair it, and keep improving. It may seem to get the job done at first, but I'm likely to be able to redesign it into something more effective and scalable.\n\r\r\rYou get better as you build more\r\rYoung children make rough Lego models, the colors don't match and the shapes are wrong. On the other hand, older children build models with careful color and shape planning.\nThe same also happens with data and algorithms. As you get to know your data and algorithms, you get to understand their limitations and strive to build something better. And as the amount of data is growing, you may need to fix and adjust your models to get better and better. In other words, the same learning curve applies to Lego building and machine learning modeling.\n\r\r\rDesign is important\r\rThe name Lego is derived from the Danish phrase \u0026lsquo;leg godt\u0026rsquo;, which means \u0026lsquo;play well\u0026rsquo;. Before I start building something with Lego, I will first decide if it's something I want to display, or something I want to play with. For display-only models, I could get away with a simpler architecture, but if it was something I wanted to play with, I knew I had to make it extra robust. After all, it would be very disappointing if the wings of my spaceship fell off while I was swooshing it around the room.\nWhen it comes to making a dashboard, Python script, or even a report, I often start by asking myself if this is something people will actually use (i.e. play with), or if it's something they want to see once and never again. From there, I plan and build accordingly.\nLego has taught me a lot about data and building models. Just like Lego:\n \u0026ldquo;To build something useful you need lots of resources, diversity, and the knowledge to build the right models in the right way.\u0026rdquo;\n ","date":"2020-08-05","permalink":"https://richardcsuwandi.github.io/blog/posts/lego/","tags":["data-science","machine-learning","programming"],"title":"Data is the New Lego"},{"categories":null,"contents":"\r\r\rMotivation\r\rAs an international student studying in China, I’ve always been fascinated by the diversity of Chinese culture and history. This motivated me to build a Chinese calligraphy classifier.\nThere are multiple styles of calligraphy, which mainly belong to different dynasties. Each of them has its way of shaping and arranging the character. For this project, I picked four styles:\n Seal Script (篆書 zhuanshu) Cursive Script (草書 caoshu) Clerical Script (隸書 lishu) Standard Script (楷書 kaishu)  If you\u0026rsquo;re interested, you can read more about these different styles here.\n\r\r\rCollecting the data\r\rTo build a calligraphy classifier, we\u0026rsquo;re going to need some examples of each style. However, I did some online search and could not find a decently made dataset for various calligraphy styles. So, I decided to create the dataset myself. Fortunately, creating my own dataset isn’t that hard, thanks to Google Images’ search functionality and some JavaScript snippets. Here’s how I did it:  I searched the images on Google Images and used this keyword format (style + “字帖網格\u0026quot;) to get the most relevant results. I used this JavaScript code to retrieve the URLs of each of the images. I downloaded the images using fast.ai’s download_images function Alternatively, I tried using this snippet to automatically download the images from Baidu Images.  \r\r\rPreprocessing the data\r\rAfter importing the data, I split the data into training and validation set with an 80:20 ratio. The images are also resized to 224 pixels, which is usually a good value for image recognition tasks. Here\u0026rsquo;s some of the images in the dataset:\nObservation: The dataset is rather ‘dirty’. Some of the images are not well-aligned and not properly cropped.\n\r\r\rBuilding the model\r\rFor the model, I use the ResNet-50 model architecture with the pre-trained weights on the ImageNet dataset. To train the layers, I use the fit_one_cycle method based on the \u0026lsquo;1 Cycle Policy\u0026rsquo;, which basically changes the learning rate over time to achieve better results.\n1  learn.fit_one_cycle(3)      epoch train_loss valid_loss accuracy     0 1.469915 0.927739 0.737500   1 1.075304 0.637498 0.790000   2 0.820588 0.574865 0.822500    After 3 epochs of fit_one_cycle, I managed to achieve an accuracy of 82% on the validation set.\n\r\r\rTuning the model\r\rBy default, the model’s initial layers are frozen to prevent modifying the pre-trained weights. I tried unfreezing all the layers and train the model again for another 2 epochs. To find the perfect learning rate, I used the lr_find and recorder.plot methods to create the learning rate plot.\nThe red dot on the graph indicates the point where the gradient is the steepest. I used that point as the first guess for the learning rate and train the model for another 2 epochs.\n1 2  min_grad_lr = learn.recorder.min_grad_lr learn.fit_one_cycle(2, min_grad_lr)      epoch train_loss valid_loss accuracy     0 0.484713 0.273136 0.885609   1 0.491012 0.287252 0.878229    \r\r\rCleaning the data\r\rfast.ai also provides a nice functionality for cleaning your data using Jupyter widgets. The ImageCleaner class displays images for re-labeling or deletion. The results of the cleaning are saved as a CSV file which I then used to load the data. I applied the same training steps as above but using the cleaned data.\n1 2  min_grad_lr = learn.recorder.min_grad_lr learn.fit_one_cycle(4, min_grad_lr)      epoch train_loss valid_loss accuracy     0 0.428563 0.235304 0.922509   1 0.398285 0.289792 0.892989   2 0.422449 0.230904 0.926199   3 0.436341 0.261377 0.915129    With only very few lines of code and very minimum efforts for data collection, I managed to achieve an accuracy of 92%. I believe with more and better-quality data, I can achieve a state-of-the-art result.\n\r\r\rInterpreting the results\r\rI used fast.ai’s ClassificationInterpretation class to interpret the results. In particular, I plot the confusion matrix to see where the model seems to be 'confused'.\nFrom the confusion matrix, it can be seen that the model does pretty well in classifying the 'zhuanshu' style. This is probably due to its unique stroke arrangements. To wrap up, I also plotted the ground truth vs. predictions by calling the learn.show_results method.\n","date":"2020-07-31","permalink":"https://richardcsuwandi.github.io/blog/posts/chinese-calligraphy/","tags":["deep-learning","machine-learning","neural-network"],"title":"I Taught My Computer to Classify Chinese Calligraphy Styles"},{"categories":null,"contents":"Data science is a lot like cooking. Although raw ingredients may be fascinating at first, the fun doesn't start until you're actually able to start slicing, dicing, and eventually serving up something delicious to devour. Most of the time, you'll end up with a dish, but in the data science world, we call it data insights. In this blog post, I want to share 5 analogies of data science to cooking which helped me understand the field better.\n\r\r\r1. Without good ingredients, you can\u0026#39;t cook a good dish\r\rJust like ingredients, data are the raw materials. And as the saying goes, \u0026quot;garbage in, garbage out\u0026quot;. Good data is key to a successful data science project. The output of your machine learning model is just as good as what you put inside it. Hence, it is important to make sure that your data contains enough relevant features and not too many irrelevant ones. Without relevant and quality data, you can't really do any useful data science.\n \u0026ldquo;Data are becoming the new raw material of business.\u0026rdquo; — Craig Mundie\n \r\r\r2. Most time and effort are spent on cleaning and preparing the ingredients\r\rThe fact that most time and effort in cooking are spent cleaning and preparing the ingredients will resonate with anyone who's had a helping hand in the kitchen. This is also true for data science. But instead of slicing, dicing, and marinating; we have feature engineering, data cleaning, and normalization. Cleaning and preparing the data is required before delivering insightful visualizations and analytics that can eventually drive data-informed business decisions.\n \u0026ldquo;Data science is 80% preparing data, 20% complaining about preparing data.\u0026rdquo;\n \r\r\r3. Different tools and techniques are needed for different recipes\r\rA cozy meal for two requires different tools compared to catering for 2,000. Similarly, processing 1,000 rows of data may run on a laptop, but processing a billion rows may require specialized distributed systems and servers. Choosing the right techniques is also important for both of these tasks. No one likes an undercooked or overcooked dish, just like underfitting and overfitting in data science.\n \u0026ldquo;There is no super algorithm that will work perfectly for all datasets.\u0026rdquo; — No Free Lunch Theorem\n \r\r\r4. Cooking is both a science and an art\r\rJust like cooking, you need certain tools and techniques, but you also need creativity and intuition. Data science doesn't exist in a vacuum, it must relate to other areas for it to have the greatest impact. Packaging the numbers creatively in a way that can be interpreted by others is crucial to getting them to see the whole picture and therefore finding the best solution. When you approach data science in a creative way, the results are often astonishing.\n \u0026ldquo;Talented data scientists leverage data that everybody sees; visionary data scientists leverage data that nobody sees.\u0026rdquo; — Vincent Granville\n \r\r\r5. You can\u0026#39;t become a great cook overnight\r\rYou can cook something by watching a video or reading the recipe from a blog. But, it doesn't really make you a great cook. Similarly, you can do some data analysis or modeling by copying code, but that won't make you a great data scientist. Likewise, completing a course or earning a certificate won't make you a great chef, and it won't make you a great data scientist either. It takes years of dedication, effort, and practice. Data science is a journey, not a destination.\n \u0026ldquo;Learning data science is like going to the gym, you only benefit if you do it consistently.\u0026rdquo; — Moez Ali, Creator of PyCaret\n ","date":"2020-07-23","permalink":"https://richardcsuwandi.github.io/blog/posts/5-reasons/","tags":["data-science","machine-learning","data-analysis"],"title":"5 Reasons Why Data Science Is Like Cooking"},{"categories":null,"contents":" \u0026ldquo;What you do every day = who you are.\u0026rdquo;\n As an aspiring data scientist, I realized the importance of the habits that we do every day, both consciously and unconsciously. During that period of realization, I found one of the best self-mastery books — \u0026quot;The 7 Habits of Highly Effective People\u0026quot; by Stephen R. Covey. As the title suggests, the book elaborates on the 7 habits of the planet's highly effective people. It's a powerful book to read for me and I'm still working on implementing these habits in my daily routine. In this blog post, I want to share with you my take on the 7 habits, seen through my lens of data science.\n\r\r\r1. Be proactive\r\rNobody understands the data better than you do. An effective data scientist should advise on how the data can be utilized as other people may not be aware of the power of data. Focus on what value the data can bring and take initiative for the greater good.\n \u0026ldquo;Reactive people are driven by feelings, by circumstances, by conditions, by their environment. Proactive people are driven by values — carefully thought about, selected and internalized values.\u0026rdquo; — Stephen Covey\n \r\r\r2. Begin with the end in mind\r\rData science isn't about reaching your goal, it's about finding the best path to your goal. An effective data scientist should have a clear idea of how the project should be carried out. Depending on the project, the best practices may vary, but without it, no project can be well-planned and well-executed. Start by figuring out what's the best output and how to do it.\n \u0026ldquo;To begin with the end in mind means to start with a clear understanding of your destination. It means to know where you're going so that you better understand where you are now and so that the steps you take are always in the right direction.\u0026rdquo; — Stephen Covey\n \r\r\r3. Put first things first\r\rData science can sometimes be overwhelming, especially when you're working on the bigger and important projects. Hence, priority must be given to tackling the most important and urgent problem. An effective data scientist understands what should have been done first. Being able to measure what's important and what's urgent, then sticking to the highest priority will pay off in the long run.\n \u0026ldquo;The key is not to prioritize what's on your schedule, but to schedule your priorities.\u0026rdquo; — Stephen Covey\n \r\r\r4. Think win-win\r\rA great data scientist is always a person with a win-win approach. Thinking about how both sides can win is crucial to building trust and ensuring long-term success. An effective data scientist knows that everybody can win! Always focus on solutions that aren't one-sided.\n \u0026ldquo;It's not your way or my way; it's a better way, a higher way.\u0026rdquo; — Stephen Covey\n \r\r\r5. Seek first to understand, then to be understood\r\rThe ability to communicate clearly is also essential for a data scientist. To do so, you must endeavor to understand your audience before attempting to make yourself understood. Empathy and open-mindedness are the keys to attaining a full understanding of people. An effective data scientist should first understand before making any assumptions. And often, seeking a different point of view could open up another better possibility.\n \u0026ldquo;Most people do not listen with the intent to understand; they listen with the intent to reply.\u0026rdquo; — Stephen Covey\n \r\r\r6. Synergize\r\rData science is not about your solo work. Teamwork is vital for a well-constructed plan to be executed with perfection, as every individual may give different and better alternatives. An effective data scientist must be able to work in sync with others. In fact, great things generally happen when people work together.\n \u0026ldquo;Synergy is better than my way or your way. It's our way.\u0026rdquo; — Stephen Covey\n \r\r\r7. Sharpen the saw\r\rData science is a huge and rapidly evolving field. A deeper understanding of data science can only be gained through continuous learning. An effective data scientist must always be ready to solve upcoming problems. Always strive for more knowledge and adapt to new challenges.\n \u0026ldquo;Unless you're continually improving your skills, you're quickly becoming irrelevant.\u0026rdquo; — Stephen Covey\n Adapting to a new habit can be challenging at first, as it requires you to get out of your comfort zone. But, instead of worrying about things you can't control, concentrate on getting control over yourself first. Remember, one step at a time!\n \u0026ldquo;Start small, make a promise and keep it. Then, make larger promises and keep them.\u0026rdquo; — Stephen Covey\n ","date":"2020-07-22","permalink":"https://richardcsuwandi.github.io/blog/posts/7-habits/","tags":["data-science","productivity","habits"],"title":"7 Habits of an Effective Data Scientist"}]