[{"categories":null,"contents":"As technology advances, AI is becoming more and more integrated into our lives. Numerous industries and companies are using AI to improve their products and services. For instance, Google uses machine learning to build personalized services and maximize user experiences. Unfortunately, it also comes with a challenge as such models need to be trained on tons of personal data in order to perform well. This certainly becomes an issue as we are increasingly concerned about privacy. Here\u0026rsquo;s where federated learning comes to play.\nIn this blog post, I will introduce the concept of federated learning, why it matters, and how it could potentially shape the future of AI.\n\r\r\rWhat is federated learning?\r\rFederated learning was first introduced by Google in 2017, in a blog post titled \u0026ldquo;Federated Learning: Collaborative Machine Learning without Centralized Training Data\u0026rdquo;. As they put it,\n \u0026ldquo;Federated learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud.\u0026rdquo;\n Thus, federated learning can be thought of as a new approach to machine learning that allows you to train models across devices without pooling the data. Instead of bringing users' data to the server, federated learning brings the machine learning model to the users' devices. With the above idea in mind, the way federated learning works is actually quite intuitive. A general federated learning framework usually involves the following steps:\n Your device downloads the current model from the server and improves it by training from data on your device Your device uploads the training results in the form of a small focused update to the server using encrypted communication The server averages the updates across all users to improve the shared model  Here, it is worth noting that federated learning is an iterative process; which means that the above steps will be repeated until a pre-defined termination criterion is met (e.g., a maximum number of iterations is reached or the model accuracy is greater than a threshold). Note: If you\u0026rsquo;re interested to learn more about how federated learning works on a technical level, you can check out this 2016 paper published by Google AI researchers. If you prefer a less technical overview, you should check out this online comic instead.\n\r\r\rWhy does federated learning matter?\r\rTraditional machine learning adopts a centralized approach which requires all the data to be brought together to a server, where the models are trained. This centralized training approach, however, is privacy-intrusive as users have to trade their privacy by sending their personal data to the server owned by the AI companies. In light of this, federated learning is basically the decentralized form of machine learning. Unlike the traditional machine learning approaches, federated learning does not require the data to be stored in the server. Instead, the data stays on each users' device and thus privacy is preserved. In such a case, users can benefit from obtaining a well-trained machine learning model without sending their sensitive personal data to the server.\n\r\r\rHow could federated learning shape the future of AI?\r\rIn recent years, data privacy has become the major obstacle for the development of AI, especially with the establishment of data protection and privacy laws like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). Thanks to federated learning, these barriers have been broken and we now have a secure way of training machine learning models within users' devices.\nFederated learning allows us to build better machine learning models that can provide personalized services and maximize user experiences. A huge amount of data are produced from user\u0026rsquo;s devices on a daily basis. These data are valuable as it contains personal information about the users and their personal interests. With federated learning, it is now possible to build such personalized models while still preserving users' privacy.\n\r\r\rWhat\u0026#39;s Next?\r\rIn this blog post, we have explored a new decentralized approach to machine learning called federated learning. I personally believe that federated learning will play an important part in shaping the future of AI. In the near future, we will see a plethora of new applications taking advantage of federated learning, enhancing user experience in a way that was not possible before. Of course, this breakthrough also comes with a unique set of challenges that AI researchers need to tackle to bring this field forward. Nevertheless, the future looks exciting if AI and privacy could go hand in hand.\n","date":"2021-08-25","permalink":"https://richardcsuwandi.github.io/blog/posts/federated-learning/","tags":["federated-learning","machine-learning","artificial-intelligence"],"title":"Why the Future of AI is Federated"},{"categories":null,"contents":"Have you ever wondered why AI has such an easy time doing stuff that we find very hard? But having a hard time doing stuff that we find very easy?\n\r\r\rMoravec\u0026#39;s Paradox\r\rIn the 1980s, an AI researcher named Hans Moravec wondered the exact same thing. As Moravec put it:\n \u0026ldquo;It is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.\u0026rdquo;\n This curious observation was later known as Moravec\u0026rsquo;s Paradox. And even though it was formulated more than 30 years ago, it is still relevant today.\nDon't get me wrong, AI certainly has come a long way. We've seen AI beating the world champion in the board game Go, the quiz game Jeopardy, the card game Poker, and the video game Dota 2. But on the flip side, AI is still having a hard time understanding a joke or interpreting people\u0026rsquo;s emotions. Therefore, the big question is:\n \u0026ldquo;Why does AI struggle with the simple?\u0026rdquo;\n \r\r\rThe reason behind Moravec\u0026#39;s Paradox\r\rAt the most basic level, the reason for Moravec's Paradox is simple: We don't know how to program general intelligence (yet). We're already good at getting AI to do specific things, but most toddler level skills require learning new things and transferring them into different contexts. And to get machines to do this is actually one of the goals of Artificial General Intelligence (AGI).\n \u0026ldquo;Today's AI is brilliant at very narrow competencies, whereas humans are good at pretty much everything.\u0026rdquo; — Dr. Sean Holden, Cambridge University\n Moravec also pointed out that the simple reason behind this is evolution. Things that seem easy to us are actually the product of thousands of years of evolution. In other words, they only seem easy to us because our species have spent thousands of years refining them.\n \u0026ldquo;The oldest human skills are largely unconscious and they appear to us to be effortless. Consequently, it should not be a surprise that the skills that appear effortless, to be computation-heavy and difficult to be reverse-engineered by a man-made AI system.\u0026rdquo;\n Moreover, the only way that we can teach an AI is by giving it a set of instructions to do a certain task. And since we've learned consciously how to do maths and win games, we know the exact steps needed to complete these tasks. Thus, we are able to teach these to an AI.\nBut how can you teach an AI to actually see, hear, or smell something? We don't know all the steps required to do these tasks consciously. In fact, we need to break these tasks into logical steps to feed into an AI. Hence, it is incredibly difficult to teach these to an AI.\n\r\r\rWhat Moravec\u0026#39;s Paradox actually taught us\r\rMoravec's Paradox has surely proven to us one thing— the fact that we've developed an AI that has beaten humans in Go or Chess doesn't mean that AGI is just around the corner. But yes, we are one step closer. It also shows why adult-level reasoning capable AI is an old hat, but AI with vision, listening, and learning capabilities are new and exciting. Of course, things are shifting, as AI is overcoming the Moravec's Paradox. More advanced AI is starting to mimic our evolutionary abilities. For instance, we've seen advancements in the Computer Vision field such as object detection and facial recognition, which could be thought of as the equivalent of sight for a computer. Also, thanks to Natural Language Processing (NLP), we now have personal assistants like Alexa that are capable of ‘hearing' and understanding us. Likewise, AI is becoming capable of speech, like we've seen in these assistants or developments like Google Duplex.\n\r\r\rThe impact of Moravec\u0026#39;s Paradox and the future of AI\r\rWhile the ultimate goal of achieving AGI remains elusive, Moravec's Paradox has brought a significant impact on our present world. Contrary to the traditional assumptions, it suggests that reasoning which is high-level in humans requires very little computational power. On the other hand, sensorimotor skills which are comparatively low-level in humans require enormous computational power. With this in mind, as computational power increases, machines could eventually match and exceed human capability.\nUltimately, AI has seen highs and lows. It's a field that is constantly saturated with ethical questions and scientific challenges. And although research on multitasking machines and AI with transferable skills is heating up, the debate remains on whether true human-level intelligence in machines is feasible (or desirable).\n \u0026ldquo;No computer has ever been designed that is ever aware of what it's doing; but most of the time, we aren't either.\u0026rdquo; — Marvin Minsky\n ","date":"2020-08-30","permalink":"https://richardcsuwandi.github.io/blog/posts/moravec/","tags":["artificial-intelligence","machine-learning","technology"],"title":"Why is AI So Smart and Yet So Dumb?"},{"categories":null,"contents":"When I was a kid, I used to love playing with Lego. My brother and I built almost all kinds of stuff with Lego — animals, cars, houses, and even spaceships. As time went on, our creations became more ambitious and realistic. There were also times when we could each have insisted that our Lego was our own, till we realized that pooling resources would eventually help us went further. We were growing up too, and as our playing became more sophisticated, we learned how to make better models.\nAs an aspiring data scientist, I realized that working with data is surprisingly a lot like my childhood Lego memories. In this blog post, I want to share some of the memories I've had that show how playing with Lego and working with data are closer than you think.\n\r\r\rExploration is the most fun part of the process\r\rWhen I was a kid, I liked to put all my Lego bricks together in a giant tub because a lot of fun in building something was searching through a sea of bricks and trying out new patterns that I didn't think about before. Anyone who deals with data knows that as much as 80% of the process is cleaning up the data and doing exploratory analysis. Personally, that's what I love about working with data — that's where I let my creativity and imagination run wild. Jumping straight into the dataset and exploring various visualizations and correlations, in search of patterns, brings me back to a childhood spent digging through a pile of Lego.\n\r\r\rTo build something useful you need lots of resources\r\rIf you don't have enough Lego bricks, chances are the things you're building aren't realistic. The model is crude, the colors don't match, and there are gaps. The same goes for machine learning models. If you don't have enough data, your models are poor, and you will encounter lots of errors.\nHowever, sometimes, I might not have the right pieces to build a model exactly the way I wanted it, so I had to search for alternatives or reconsider how to build my Lego model. Hence, I learned a new way of using what I had. Similarly, as long as you are creative about where you look, there are always insights to be gained from even the most limited data.\n\r\r\rA good quality model needs a diversity of resources\r\rTo build a good quality Lego model, you also need a diversity of bricks. Models built with only the basic 2x4 bricks are rough and inaccurate. This is where it was so useful to get Lego from friends and family. As our family and friends gave us more Lego bricks, we got more diverse bricks that helped us create more accurate models.\nThis may also be a harsh childhood truth, that the children with the most Lego, the best pieces, and the most time to play create the best models. The same harsh truth applies to any machine learning projects. Projects with the biggest data volumes, the most diverse data, and the best teams to use the data would create the most accurate models.\n\r\r\rBoth require iterative thinking\r\rThe beauty of Lego is that you're not limited to what's on the box. Rebuilding something and refining it each time requires iterative thinking. When it comes to working with data, there are also plenty of opportunities to iterate.\nWhen I get a \u0026ldquo;decent enough\u0026rdquo; solution, whether it's a dashboard or a Python script, I still find time to break it, repair it, and keep improving. It may seem to get the job done at first, but I'm likely to be able to redesign it into something more effective and scalable.\n\r\r\rYou get better as you build more\r\rYoung children make rough Lego models, the colors don't match and the shapes are wrong. On the other hand, older children build models with careful color and shape planning.\nThe same also happens with data and algorithms. As you get to know your data and algorithms, you get to understand their limitations and strive to build something better. And as the amount of data is growing, you may need to fix and adjust your models to get better and better. In other words, the same learning curve applies to Lego building and machine learning modeling.\n\r\r\rDesign is important\r\rThe name Lego is derived from the Danish phrase \u0026lsquo;leg godt\u0026rsquo;, which means \u0026lsquo;play well\u0026rsquo;. Before I start building something with Lego, I will first decide if it's something I want to display, or something I want to play with. For display-only models, I could get away with a simpler architecture, but if it was something I wanted to play with, I knew I had to make it extra robust. After all, it would be very disappointing if the wings of my spaceship fell off while I was swooshing it around the room.\nWhen it comes to making a dashboard, Python script, or even a report, I often start by asking myself if this is something people will actually use (i.e. play with), or if it's something they want to see once and never again. From there, I plan and build accordingly.\nLego has taught me a lot about data and building models. Just like Lego:\n \u0026ldquo;To build something useful you need lots of resources, diversity, and the knowledge to build the right models in the right way.\u0026rdquo;\n ","date":"2020-08-05","permalink":"https://richardcsuwandi.github.io/blog/posts/lego/","tags":["data-science","machine-learning","programming"],"title":"Data is the New Lego"},{"categories":null,"contents":"Data science is a lot like cooking. Although raw ingredients may be fascinating at first, the fun doesn't start until you're actually able to start slicing, dicing, and eventually serving up something delicious to devour. Most of the time, you'll end up with a dish, but in the data science world, we call it data insights. In this blog post, I want to share 5 analogies of data science to cooking which helped me understand the field better.\n\r\r\r1. Without good ingredients, you can\u0026#39;t cook a good dish\r\rJust like ingredients, data are the raw materials. And as the saying goes, \u0026quot;garbage in, garbage out\u0026quot;. Good data is key to a successful data science project. The output of your machine learning model is just as good as what you put inside it. Hence, it is important to make sure that your data contains enough relevant features and not too many irrelevant ones. Without relevant and quality data, you can't really do any useful data science.\n \u0026ldquo;Data are becoming the new raw material of business.\u0026rdquo; — Craig Mundie\n \r\r\r2. Most time and effort are spent on cleaning and preparing the ingredients\r\rThe fact that most time and effort in cooking are spent cleaning and preparing the ingredients will resonate with anyone who's had a helping hand in the kitchen. This is also true for data science. But instead of slicing, dicing, and marinating; we have feature engineering, data cleaning, and normalization. Cleaning and preparing the data is required before delivering insightful visualizations and analytics that can eventually drive data-informed business decisions.\n \u0026ldquo;Data science is 80% preparing data, 20% complaining about preparing data.\u0026rdquo;\n \r\r\r3. Different tools and techniques are needed for different recipes\r\rA cozy meal for two requires different tools compared to catering for 2,000. Similarly, processing 1,000 rows of data may run on a laptop, but processing a billion rows may require specialized distributed systems and servers. Choosing the right techniques is also important for both of these tasks. No one likes an undercooked or overcooked dish, just like underfitting and overfitting in data science.\n \u0026ldquo;There is no super algorithm that will work perfectly for all datasets.\u0026rdquo; — No Free Lunch Theorem\n \r\r\r4. Cooking is both a science and an art\r\rJust like cooking, you need certain tools and techniques, but you also need creativity and intuition. Data science doesn't exist in a vacuum, it must relate to other areas for it to have the greatest impact. Packaging the numbers creatively in a way that can be interpreted by others is crucial to getting them to see the whole picture and therefore finding the best solution. When you approach data science in a creative way, the results are often astonishing.\n \u0026ldquo;Talented data scientists leverage data that everybody sees; visionary data scientists leverage data that nobody sees.\u0026rdquo; — Vincent Granville\n \r\r\r5. You can\u0026#39;t become a great cook overnight\r\rYou can cook something by watching a video or reading the recipe from a blog. But, it doesn't really make you a great cook. Similarly, you can do some data analysis or modeling by copying code, but that won't make you a great data scientist. Likewise, completing a course or earning a certificate won't make you a great chef, and it won't make you a great data scientist either. It takes years of dedication, effort, and practice. Data science is a journey, not a destination.\n \u0026ldquo;Learning data science is like going to the gym, you only benefit if you do it consistently.\u0026rdquo; — Moez Ali, Creator of PyCaret\n ","date":"2020-07-23","permalink":"https://richardcsuwandi.github.io/blog/posts/5-reasons/","tags":["data-science","machine-learning","data-analysis"],"title":"5 Reasons Why Data Science Is Like Cooking"},{"categories":null,"contents":" \u0026ldquo;What you do every day = who you are.\u0026rdquo;\n As an aspiring data scientist, I realized the importance of the habits that we do every day, both consciously and unconsciously. During that period of realization, I found one of the best self-mastery books — \u0026quot;The 7 Habits of Highly Effective People\u0026quot; by Stephen R. Covey. As the title suggests, the book elaborates on the 7 habits of the planet's highly effective people. It's a powerful book to read for me and I'm still working on implementing these habits in my daily routine. In this blog post, I want to share with you my take on the 7 habits, seen through my lens of data science.\n\r\r\r1. Be proactive\r\rNobody understands the data better than you do. An effective data scientist should advise on how the data can be utilized as other people may not be aware of the power of data. Focus on what value the data can bring and take initiative for the greater good.\n \u0026ldquo;Reactive people are driven by feelings, by circumstances, by conditions, by their environment. Proactive people are driven by values — carefully thought about, selected and internalized values.\u0026rdquo; — Stephen Covey\n \r\r\r2. Begin with the end in mind\r\rData science isn't about reaching your goal, it's about finding the best path to your goal. An effective data scientist should have a clear idea of how the project should be carried out. Depending on the project, the best practices may vary, but without it, no project can be well-planned and well-executed. Start by figuring out what's the best output and how to do it.\n \u0026ldquo;To begin with the end in mind means to start with a clear understanding of your destination. It means to know where you're going so that you better understand where you are now and so that the steps you take are always in the right direction.\u0026rdquo; — Stephen Covey\n \r\r\r3. Put first things first\r\rData science can sometimes be overwhelming, especially when you're working on the bigger and important projects. Hence, priority must be given to tackling the most important and urgent problem. An effective data scientist understands what should have been done first. Being able to measure what's important and what's urgent, then sticking to the highest priority will pay off in the long run.\n \u0026ldquo;The key is not to prioritize what's on your schedule, but to schedule your priorities.\u0026rdquo; — Stephen Covey\n \r\r\r4. Think win-win\r\rA great data scientist is always a person with a win-win approach. Thinking about how both sides can win is crucial to building trust and ensuring long-term success. An effective data scientist knows that everybody can win! Always focus on solutions that aren't one-sided.\n \u0026ldquo;It's not your way or my way; it's a better way, a higher way.\u0026rdquo; — Stephen Covey\n \r\r\r5. Seek first to understand, then to be understood\r\rThe ability to communicate clearly is also essential for a data scientist. To do so, you must endeavor to understand your audience before attempting to make yourself understood. Empathy and open-mindedness are the keys to attaining a full understanding of people. An effective data scientist should first understand before making any assumptions. And often, seeking a different point of view could open up another better possibility.\n \u0026ldquo;Most people do not listen with the intent to understand; they listen with the intent to reply.\u0026rdquo; — Stephen Covey\n \r\r\r6. Synergize\r\rData science is not about your solo work. Teamwork is vital for a well-constructed plan to be executed with perfection, as every individual may give different and better alternatives. An effective data scientist must be able to work in sync with others. In fact, great things generally happen when people work together.\n \u0026ldquo;Synergy is better than my way or your way. It's our way.\u0026rdquo; — Stephen Covey\n \r\r\r7. Sharpen the saw\r\rData science is a huge and rapidly evolving field. A deeper understanding of data science can only be gained through continuous learning. An effective data scientist must always be ready to solve upcoming problems. Always strive for more knowledge and adapt to new challenges.\n \u0026ldquo;Unless you're continually improving your skills, you're quickly becoming irrelevant.\u0026rdquo; — Stephen Covey\n Adapting to a new habit can be challenging at first, as it requires you to get out of your comfort zone. But, instead of worrying about things you can't control, concentrate on getting control over yourself first. Remember, one step at a time!\n \u0026ldquo;Start small, make a promise and keep it. Then, make larger promises and keep them.\u0026rdquo; — Stephen Covey\n ","date":"2020-07-22","permalink":"https://richardcsuwandi.github.io/blog/posts/7-habits/","tags":["data-science","productivity","habits"],"title":"7 Habits of an Effective Data Scientist"}]